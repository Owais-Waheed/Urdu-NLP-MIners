import streamlit as st
import matplotlib.pyplot as plt
from newspaper import Article
import nltk
import io
from io import StringIO
import PyPDF2
from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer, AutoModelForCausalLM
from langdetect import detect
import torch
# from docx import Document

# Download necessary NLTK data
try:
    nltk.data.find('tokenizers/punkt')
except LookupError:
    nltk.download('punkt')

# Set page configuration
st.set_page_config(
    page_title="Ø§Ø±Ø¯Ùˆ Ù†ÛŒÙˆØ² Ø§ÛŒÚ©Ø³Ù¾Ø±ÛŒØ³",
    page_icon="ğŸ“°"
)

# Title and description
st.title("Ø§Ø±Ø¯Ùˆ Ù†ÛŒÙˆØ² Ø§ÛŒÚ©Ø³Ù¾Ø±ÛŒØ³")
st.write("Ø§Ø±Ø¯Ùˆ Ø®Ø¨Ø±ÙˆÚº Ú©Ø§ Ø®Ù„Ø§ØµÛ Ø§ÙˆØ± Ø¬Ø°Ø¨Ø§ØªÛŒ ØªØ¬Ø²ÛŒÛ")
st.write("##")

# Cache the model loading to improve performance

@st.cache_resource
def load_sentiment_model():
    model_path = "./urdusenti"
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForSequenceClassification.from_pretrained(model_path)
    return pipeline("sentiment-analysis", model=model, tokenizer=tokenizer)

# @st.cache_resource
# def load_summarizer_model():
#     # Using mT5 model which supports Urdu
#     model_name = "google/mt5-small"
#     return pipeline("summarization", model=model_name)

# # Load the summarizer model
# summarizer = load_summarizer_model()
# classifier = load_sentiment_model()
# summarizer, classifier = 0,0

# # Function to extract text from docx with fallback
# def extract_text_from_docx(file):
#     doc = Document(file)
#     return '\n'.join([paragraph.text for paragraph in doc.paragraphs])

# Function to extract text from various file types
def extract_text_from_file(file):
    file_type = file.name.split('.')[-1].lower()
    
    if file_type == 'txt':
        return StringIO(file.getvalue().decode('utf-8')).read()
    # elif file_type == 'docx':
    #     return extract_text_from_docx(file)
    elif file_type == 'pdf':
        pdf_reader = PyPDF2.PdfReader(io.BytesIO(file.getvalue()))
        text = ""
        for page in pdf_reader.pages:
            text += page.extract_text() + "\n"
        return text
    else:
        st.error("ÙØ§Ø¦Ù„ Ú©ÛŒ Ù‚Ø³Ù… Ù…Ø¹Ø§ÙˆÙ† Ù†ÛÛŒÚº ÛÛ’Û” Ø¨Ø±Ø§Ø¦Û’ Ù…ÛØ±Ø¨Ø§Ù†ÛŒ TXTØŒ DOCX ÛŒØ§ PDF ÙØ§Ø¦Ù„ Ø§Ù¾Ù„ÙˆÚˆ Ú©Ø±ÛŒÚºÛ”")
        return ""


summary_model = "./gemma"
summarizer = AutoModelForCausalLM.from_pretrained(summary_model)
sum_tokenizer = AutoTokenizer.from_pretrained(summary_model)
classifier = load_sentiment_model()

def get_summary(text, max_chunk_words=400, max_new_tokens=150, model=None, tokenizer=None):
    import nltk
    nltk.download('punkt', quiet=True)

    # Split into sentences and form chunks
    sentences = nltk.sent_tokenize(text)
    chunks = []
    current_chunk = []
    current_length = 0

    for sentence in sentences:
        words = sentence.split()
        if current_length + len(words) <= max_chunk_words:
            current_chunk.append(sentence)
            current_length += len(words)
        else:
            chunks.append(" ".join(current_chunk))
            current_chunk = [sentence]
            current_length = len(words)

    if current_chunk:
        chunks.append(" ".join(current_chunk))

    # Process each chunk with the Gemma model
    all_summaries = []
    for chunk in chunks:
        prompt = f"Ù…Ù†Ø¯Ø±Ø¬Û Ø°ÛŒÙ„ Ø®Ø¨Ø± Ú©Ø§ Ø®Ù„Ø§ØµÛ Ù„Ú©Ú¾ÛŒÚº:\n\n{chunk}\n\nØ®Ù„Ø§ØµÛ:"
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

        output = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=0.7,
            do_sample=True,
            top_k=50,
            top_p=0.95
        )

        decoded = tokenizer.decode(output[0], skip_special_tokens=True)
        summary_part = decoded.split("Ø®Ù„Ø§ØµÛ:")[-1].strip()
        all_summaries.append(summary_part)

    return " ".join(all_summaries)

# Function to analyze sentiment using LLM
def get_sentiment(text, classifier):
    # Load the sentiment analysis model
    
    # Process long texts in chunks and aggregate results
    max_token_length = 512
    
    # If text is short enough, analyze directly
    if len(text.split()) < max_token_length:
        result = classifier(text)
        label = result[0]['label']
        score = result[0]['score']
        
        # Map the result to Urdu sentiment labels
        sentiment_mapping = {
            '1 star': "Ø§Ù†ØªÛØ§Ø¦ÛŒ Ù…Ù†ÙÛŒ",
            '2 stars': "Ù…Ù†ÙÛŒ",
            '3 stars': "ØºÛŒØ± Ø¬Ø§Ù†Ø¨Ø¯Ø§Ø±",
            '4 stars': "Ù…Ø«Ø¨Øª",
            '5 stars': "Ø§Ù†ØªÛØ§Ø¦ÛŒ Ù…Ø«Ø¨Øª"
        }
        
        return sentiment_mapping.get(label, "ØºÛŒØ± Ø¬Ø§Ù†Ø¨Ø¯Ø§Ø±"), score
    
    # For longer texts, split and analyze in chunks
    sentences = nltk.sent_tokenize(text)
    chunks = []
    current_chunk = []
    current_length = 0
    
    for sentence in sentences:
        sentence_length = len(sentence.split())
        if current_length + sentence_length <= max_token_length:
            current_chunk.append(sentence)
            current_length += sentence_length
        else:
            chunks.append(" ".join(current_chunk))
            current_chunk = [sentence]
            current_length = sentence_length
    
    if current_chunk:
        chunks.append(" ".join(current_chunk))
    
    # Analyze each chunk
    overall_score = 0
    for chunk in chunks:
        result = classifier(chunk)
        # Convert rating to numeric score (1-5)
        rating = int(result[0]['label'].split()[0])
        overall_score += (rating - 1) / 4  # Normalize to 0-1 range
    
    # Average the scores
    if chunks:
        overall_score /= len(chunks)
    
    # Map to Urdu sentiment
    if overall_score < 0.3:
        return "Ù…Ù†ÙÛŒ", overall_score
    elif overall_score > 0.7:
        return "Ù…Ø«Ø¨Øª", overall_score
    else:
        return "ØºÛŒØ± Ø¬Ø§Ù†Ø¨Ø¯Ø§Ø±", overall_score

# Create tabs for different input methods
tab1, tab2, tab3 = st.tabs(["ÛŒÙˆ Ø¢Ø± Ø§ÛŒÙ„", "ÙØ§Ø¦Ù„ Ø§Ù¾Ù„ÙˆÚˆ", "Ù…ØªÙ† Ø¯Ø§Ø®Ù„ Ú©Ø±ÛŒÚº"])

with tab1:
    with st.form("url_form", clear_on_submit=True):
        url_input = st.text_input("Ø®Ø¨Ø± Ú©Ø§ ÛŒÙˆ Ø¢Ø± Ø§ÛŒÙ„ Ø¯Ø§Ø®Ù„ Ú©Ø±ÛŒÚº")
        url_submit = st.form_submit_button("ØªÙ„Ø§Ø´ Ú©Ø±ÛŒÚº")
    
    if url_submit and url_input:
        try:
            with st.spinner("Ø®Ø¨Ø± Ù„ÙˆÚˆ ÛÙˆ Ø±ÛÛŒ ÛÛ’..."):
                article = Article(url_input)
                article.download()
                article.parse()
                
                # Check if the article is in Urdu
                try:
                    language = detect(article.text[:100])
                    if language != 'ur':
                        st.warning("ÛŒÛ Ø®Ø¨Ø± Ø§Ø±Ø¯Ùˆ Ù…ÛŒÚº Ù†ÛÛŒÚº ÛÛ’Û” Ù†ØªØ§Ø¦Ø¬ Ø¯Ø±Ø³Øª Ù†ÛÛŒÚº ÛÙˆ Ø³Ú©ØªÛ’Û”")
                except:
                    pass
                
                # Display article details
                st.subheader("Ø®Ø¨Ø± Ú©ÛŒ ØªÙØµÛŒÙ„Ø§Øª:")
                st.write(f"### {article.title}")
                
                if article.top_image:
                    st.image(article.top_image)
                
                with st.spinner("Ø®Ù„Ø§ØµÛ Ø¨Ù†Ø§ÛŒØ§ Ø¬Ø§ Ø±ÛØ§ ÛÛ’..."):
                    summary = get_summary(article.text, summarizer=summarizer, tokenizer=sum_tokenizer)
                    st.subheader("Ø®Ø¨Ø± Ú©Ø§ Ø®Ù„Ø§ØµÛ:")
                    st.write(summary)
                
                with st.spinner("Ø¬Ø°Ø¨Ø§ØªÛŒ ØªØ¬Ø²ÛŒÛ Ú©ÛŒØ§ Ø¬Ø§ Ø±ÛØ§ ÛÛ’..."):
                    sentiment, score = get_sentiment(article.text, classifier=classifier)
                    st.subheader("Ø¬Ø°Ø¨Ø§ØªÛŒ ØªØ¬Ø²ÛŒÛ:")
                    st.write(f"ÛŒÛ Ø®Ø¨Ø± **{sentiment}** ÛÛ’")
                    st.progress(score)
                
        except Exception as e:
            st.error(f"Ú©ÙˆØ¦ÛŒ Ø®Ø±Ø§Ø¨ÛŒ ÛÙˆØ¦ÛŒ ÛÛ’Û” Ø¯Ø±Ø³Øª ÛŒÙˆ Ø¢Ø± Ø§ÛŒÙ„ Ø¯Ø§Ø®Ù„ Ú©Ø±ÛŒÚºÛ” Ø®Ø±Ø§Ø¨ÛŒ: {str(e)}")

with tab2:
    uploaded_file = st.file_uploader("Ø§Ø±Ø¯Ùˆ Ø®Ø¨Ø± Ú©ÛŒ ÙØ§Ø¦Ù„ Ø§Ù¾Ù„ÙˆÚˆ Ú©Ø±ÛŒÚº", type=["txt", "pdf"])
    
    if uploaded_file is not None:
        with st.spinner("ÙØ§Ø¦Ù„ Ú©Ùˆ Ù¾Ú‘Ú¾Ø§ Ø¬Ø§ Ø±ÛØ§ ÛÛ’..."):
            text = extract_text_from_file(uploaded_file)
            
            if text:
                with st.spinner("Ø®Ù„Ø§ØµÛ Ø¨Ù†Ø§ÛŒØ§ Ø¬Ø§ Ø±ÛØ§ ÛÛ’..."):
                    summary = get_summary(text, summarizer=summarizer)
                    st.subheader("Ø¯Ø³ØªØ§ÙˆÛŒØ² Ú©Ø§ Ø®Ù„Ø§ØµÛ:")
                    st.write(summary)
                
                with st.spinner("Ø¬Ø°Ø¨Ø§ØªÛŒ ØªØ¬Ø²ÛŒÛ Ú©ÛŒØ§ Ø¬Ø§ Ø±ÛØ§ ÛÛ’..."):
                    sentiment, score = get_sentiment(text, classifier=classifier)
                    st.subheader("Ø¬Ø°Ø¨Ø§ØªÛŒ ØªØ¬Ø²ÛŒÛ:")
                    st.write(f"ÛŒÛ Ø¯Ø³ØªØ§ÙˆÛŒØ² **{sentiment}** ÛÛ’")
                    st.progress(score)

with tab3:
    text_input = st.text_area("Ø§Ø±Ø¯Ùˆ Ù…ØªÙ† Ø¯Ø§Ø®Ù„ Ú©Ø±ÛŒÚº", height=300)
    analyze_button = st.button("ØªØ¬Ø²ÛŒÛ Ú©Ø±ÛŒÚº")
    
    if analyze_button and text_input:
        with st.spinner("Ø®Ù„Ø§ØµÛ Ø¨Ù†Ø§ÛŒØ§ Ø¬Ø§ Ø±ÛØ§ ÛÛ’..."):
            summary = get_summary(text_input)
            st.subheader("Ù…ØªÙ† Ú©Ø§ Ø®Ù„Ø§ØµÛ:")
            st.write(summary)
        
        with st.spinner("Ø¬Ø°Ø¨Ø§ØªÛŒ ØªØ¬Ø²ÛŒÛ Ú©ÛŒØ§ Ø¬Ø§ Ø±ÛØ§ ÛÛ’..."):
            sentiment, score = get_sentiment(text_input, classifier=classifier)
            st.subheader("Ø¬Ø°Ø¨Ø§ØªÛŒ ØªØ¬Ø²ÛŒÛ:")
            st.write(f"ÛŒÛ Ù…ØªÙ† **{sentiment}** ÛÛ’")
            st.progress(score)

# Add a sidebar for model configuration
with st.sidebar:
    st.header("Ù…Ø§ÚˆÙ„ Ú©ÛŒ ØªØ±ØªÛŒØ¨Ø§Øª")
    st.info("ÛŒÛ Ø§ÛŒÙ¾Ù„ÛŒÚ©ÛŒØ´Ù† Ø§Ø±Ø¯Ùˆ Ù†ÛŒÙˆØ² Ú©Ø§ Ø®Ù„Ø§ØµÛ Ø§ÙˆØ± Ø¬Ø°Ø¨Ø§ØªÛŒ ØªØ¬Ø²ÛŒÛ Ú©Ø±ØªÛŒ ÛÛ’ ")
    st.write("**Ø§ÛÙ… ÛØ¯Ø§ÛŒØ§Øª:**")
    st.write("1. Ù¾ÛÙ„ÛŒ Ø¨Ø§Ø± Ú†Ù„Ø§Ù†Û’ Ù¾Ø±ØŒ Ù…Ø§ÚˆÙ„ ÚˆØ§Ø¤Ù† Ù„ÙˆÚˆ Ú©Ø±Ù†Û’ Ù…ÛŒÚº Ú†Ù†Ø¯ Ù…Ù†Ù¹ Ù„Ú¯ Ø³Ú©ØªÛ’ ÛÛŒÚº")
    st.write("2. Ø¨Ú‘Û’ Ù…ØªÙ† Ú©Û’ ØªØ¬Ø²ÛŒÛ’ Ù…ÛŒÚº Ø²ÛŒØ§Ø¯Û ÙˆÙ‚Øª Ù„Ú¯ Ø³Ú©ØªØ§ ÛÛ’")
    # st.write("3. Ø¨Ø±Ø§Û Ø±Ø§Ø³Øª Ø§Ø±Ø¯Ùˆ Ù…ØªÙ† Ø³Ø¨ Ø³Û’ Ø¨ÛØªØ± Ù†ØªØ§Ø¦Ø¬ Ø¯ÛŒØªØ§ ÛÛ’")

# Footer
st.markdown("---")
st.caption("Ø§Ø±Ø¯Ùˆ Ù†ÛŒÙˆØ² Ø§ÛŒÚ©Ø³Ù¾Ø±ÛŒØ³ - Ø®Ø¨Ø±ÙˆÚº Ú©Ø§ Ø®Ù„Ø§ØµÛ Ø§ÙˆØ± Ø¬Ø°Ø¨Ø§ØªÛŒ ØªØ¬Ø²ÛŒÛ")