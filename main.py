import streamlit as st
import matplotlib.pyplot as plt
from newspaper import Article
# import nltk 
import io
from io import StringIO
import PyPDF2
from transformers import AutoModelForSequenceClassification, AutoTokenizer, AutoModelForCausalLM
# from langdetect import detect
import torch
# from llm_guard.input_scanners import PromptInjection
# from llm_guard.output_scanners import Toxicity
# from llm_guard import scan_output
# from docx import Document

# def validate_summary(summary, original_text):
#     scanners = [
#         # Profanity(threshold=0.8, languages=["ur", "en"]),
#         Toxicity(threshold=0.7, languages=["ur", "en"]),
#         # SensitiveTopics(languages=["ur", "en"]),
#         # Hallucination(reference=original_text),
#     ]

#     issues, _ = scan_output(summary, scanners)
#     return issues

# Download necessary NLTK data
# nltk.download('punkt')
# from nltk.tokenize import sent_tokenize
    

# Set page configuration
st.set_page_config(
    page_title="Ø§Ø±Ø¯Ùˆ Ù†ÛŒÙˆØ² Ø§ÛŒÚ©Ø³Ù¾Ø±ÛŒØ³",
    page_icon="ğŸ“°"
)

# Title and description
st.title("Ø§Ø±Ø¯Ùˆ Ù†ÛŒÙˆØ² Ø§ÛŒÚ©Ø³Ù¾Ø±ÛŒØ³")
st.write("Ø§Ø±Ø¯Ùˆ Ø®Ø¨Ø±ÙˆÚº Ú©Ø§ Ø®Ù„Ø§ØµÛ Ø§ÙˆØ± Ø¬Ø°Ø¨Ø§ØªÛŒ ØªØ¬Ø²ÛŒÛ")
st.write("##")

# Cache the model loading to improve performance

@st.cache_resource
def load_sentiment_model():
    model_path = "./urdusenti"
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForSequenceClassification.from_pretrained(model_path).to("cpu")
    return model, tokenizer

# @st.cache_resource
# def load_summarizer_model():
#     # Using mT5 model which supports Urdu
#     model_name = "google/mt5-small"
#     return pipeline("summarization", model=model_name)

# # Load the summarizer model
# summarizer = load_summarizer_model()
# classifier = load_sentiment_model()
# summarizer, classifier = 0,0

# # Function to extract text from docx with fallback
# def extract_text_from_docx(file):
#     doc = Document(file)
#     return '\n'.join([paragraph.text for paragraph in doc.paragraphs])

# Function to extract text from various file types
def extract_text_from_file(file):
    file_type = file.name.split('.')[-1].lower()
    
    if file_type == 'txt':
        return StringIO(file.getvalue().decode('utf-8')).read()
    # elif file_type == 'docx':
    #     return extract_text_from_docx(file)
    elif file_type == 'pdf':
        pdf_reader = PyPDF2.PdfReader(io.BytesIO(file.getvalue()))
        text = ""
        for page in pdf_reader.pages:
            text += page.extract_text() + "\n"
        return text
    else:
        st.error("ÙØ§Ø¦Ù„ Ú©ÛŒ Ù‚Ø³Ù… Ù…Ø¹Ø§ÙˆÙ† Ù†ÛÛŒÚº ÛÛ’Û” Ø¨Ø±Ø§Ø¦Û’ Ù…ÛØ±Ø¨Ø§Ù†ÛŒ TXTØŒ DOCX ÛŒØ§ PDF ÙØ§Ø¦Ù„ Ø§Ù¾Ù„ÙˆÚˆ Ú©Ø±ÛŒÚºÛ”")
        return ""


summary_model = "./gemma"
# summarizer = AutoModelForCausalLM.from_pretrained(summary_model).to("cpu")
sum_tokenizer = AutoTokenizer.from_pretrained(summary_model)
summarizer = AutoModelForCausalLM.from_pretrained(summary_model,
    torch_dtype=torch.float32,
    low_cpu_mem_usage=False).to("cpu")
classifier, class_tokenizer = load_sentiment_model()

def get_summary(text, max_chunk_words=400, max_new_tokens=150, model=None, tokenizer=None):
    

    # Split into sentences and form chunks
    # Manually split the text into sentences using simple punctuation rules
    sentences = text.split('Û”')  # Split on Urdu full stops (Û”)
    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]

    chunks = []
    current_chunk = []
    current_length = 0


    for sentence in sentences:
        words = sentence.split()
        if current_length + len(words) <= max_chunk_words:
            current_chunk.append(sentence)
            current_length += len(words)
        else:
            chunks.append(" ".join(current_chunk))
            current_chunk = [sentence]
            current_length = len(words)

    if current_chunk:
        chunks.append(" ".join(current_chunk))

    # Process each chunk with the Gemma model
    all_summaries = []
    for chunk in chunks:
        prompt = f"Ù…Ù†Ø¯Ø±Ø¬Û Ø°ÛŒÙ„ Ø®Ø¨Ø± Ú©Ø§ Ø®Ù„Ø§ØµÛ Ù„Ú©Ú¾ÛŒÚº:\n\n{chunk}\n\nØ®Ù„Ø§ØµÛ:"
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

        output = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            temperature=0.7,
            do_sample=True,
            top_k=50,
            top_p=0.95
        )

        decoded = tokenizer.decode(output[0], skip_special_tokens=True)
        summary_part = decoded.split("Ø®Ù„Ø§ØµÛ:")[-1].strip()
        all_summaries.append(summary_part)

    return " ".join(all_summaries)

# Function to analyze sentiment using LLM

def get_sentiment(text, tokenizer, model, threshold=0.6):
    model.eval()
    max_token_length = 512

    # Urdu sentiment label mapping (index: label)
    urdu_labels = [
          # 0
        "Ù…Ù†ÙÛŒ",         # 1
        "ØºÛŒØ± Ø¬Ø§Ù†Ø¨Ø¯Ø§Ø±",  # 2
        "Ù…Ø«Ø¨Øª",         # 3
           # 4
    ]

    # Split long text into chunks
    # Manually split the text into sentences using simple punctuation rules
    sentences = text.split('Û”')  # Split on Urdu full stops (Û”)
    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]

    chunks = []
    current_chunk = []
    current_length = 0


    for sentence in sentences:
        sentence_length = len(sentence.split())
        if current_length + sentence_length <= max_token_length:
            current_chunk.append(sentence)
            current_length += sentence_length
        else:
            chunks.append(" ".join(current_chunk))
            current_chunk = [sentence]
            current_length = sentence_length

    if current_chunk:
        chunks.append(" ".join(current_chunk))

    sigmoid = torch.nn.Sigmoid()
    all_probs = []

    # Run model on each chunk and collect probabilities
    for chunk in chunks:
        encoding = tokenizer(chunk, padding=True, truncation=True, max_length=512, return_tensors='pt')
        with torch.no_grad():
            outputs = model(**encoding)
        probs = sigmoid(outputs.logits)
        all_probs.append(probs)

    # Average predictions
    avg_probs = torch.mean(torch.stack(all_probs), dim=0).flatten()

    # Apply threshold and map to Urdu labels
    predicted_labels = (avg_probs >= threshold).int().tolist()
    urdu_predictions = [urdu_labels[i] for i, pred in enumerate(predicted_labels) if pred == 1]
    scores = avg_probs.tolist()

    return urdu_predictions[0], scores


# Create tabs for different input methods
tab1, tab2, tab3 = st.tabs(["ÛŒÙˆ Ø¢Ø± Ø§ÛŒÙ„", "ÙØ§Ø¦Ù„ Ø§Ù¾Ù„ÙˆÚˆ", "Ù…ØªÙ† Ø¯Ø§Ø®Ù„ Ú©Ø±ÛŒÚº"])

with tab1:
    with st.form("url_form", clear_on_submit=True):
        url_input = st.text_input("Ø®Ø¨Ø± Ú©Ø§ ÛŒÙˆ Ø¢Ø± Ø§ÛŒÙ„ Ø¯Ø§Ø®Ù„ Ú©Ø±ÛŒÚº")
        url_submit = st.form_submit_button("ØªÙ„Ø§Ø´ Ú©Ø±ÛŒÚº")
    
    if url_submit and url_input:
        try:
            with st.spinner("Ø®Ø¨Ø± Ù„ÙˆÚˆ ÛÙˆ Ø±ÛÛŒ ÛÛ’..."):
                article = Article(url_input)
                article.download()
                article.parse()
                
                # Check if the article is in Urdu
                try:
                    language = detect(article.text[:100])
                    if language != 'ur':
                        st.warning("ÛŒÛ Ø®Ø¨Ø± Ø§Ø±Ø¯Ùˆ Ù…ÛŒÚº Ù†ÛÛŒÚº ÛÛ’Û” Ù†ØªØ§Ø¦Ø¬ Ø¯Ø±Ø³Øª Ù†ÛÛŒÚº ÛÙˆ Ø³Ú©ØªÛ’Û”")
                except:
                    pass
                
                # Display article details
                st.subheader("Ø®Ø¨Ø± Ú©ÛŒ ØªÙØµÛŒÙ„Ø§Øª:")
                st.write(f"### {article.title}")
                
                if article.top_image:
                    st.image(article.top_image)
                
                with st.spinner("Ø®Ù„Ø§ØµÛ Ø¨Ù†Ø§ÛŒØ§ Ø¬Ø§ Ø±ÛØ§ ÛÛ’..."):
                    summary = get_summary(article.text, model=summarizer, tokenizer=sum_tokenizer)
                    # issues = validate_summary(summary, original_text=article.text)

                    # if issues:
                    #     st.warning("Ø®Ù„Ø§ØµÛ’ Ù…ÛŒÚº Ú©Ú†Ú¾ Ù…Ù…Ú©Ù†Û Ù…Ø³Ø§Ø¦Ù„ Ù¾Ø§Ø¦Û’ Ú¯Ø¦Û’:")
                    #     for issue in issues:
                    #         st.markdown(f"- âš ï¸ {issue}")
                    # else:
                    #     st.success("Ø®Ù„Ø§ØµÛ Ù…Ø­ÙÙˆØ¸ Ø§ÙˆØ± Ù…Ù†Ø§Ø³Ø¨ ÛÛ’Û”")

                    st.subheader("Ø®Ø¨Ø± Ú©Ø§ Ø®Ù„Ø§ØµÛ:")
                    st.write(summary)
                
                with st.spinner("Ø¬Ø°Ø¨Ø§ØªÛŒ ØªØ¬Ø²ÛŒÛ Ú©ÛŒØ§ Ø¬Ø§ Ø±ÛØ§ ÛÛ’..."):
                    sentiment, score = get_sentiment(article.text, model=classifier, tokenizer=class_tokenizer)
                    st.subheader("Ø¬Ø°Ø¨Ø§ØªÛŒ ØªØ¬Ø²ÛŒÛ:")
                    st.write(f"ÛŒÛ Ø®Ø¨Ø± **{sentiment}** ÛÛ’")
                    # st.progress(score)
                
        except Exception as e:
            st.error(f"Ú©ÙˆØ¦ÛŒ Ø®Ø±Ø§Ø¨ÛŒ ÛÙˆØ¦ÛŒ ÛÛ’Û” Ø¯Ø±Ø³Øª ÛŒÙˆ Ø¢Ø± Ø§ÛŒÙ„ Ø¯Ø§Ø®Ù„ Ú©Ø±ÛŒÚºÛ” Ø®Ø±Ø§Ø¨ÛŒ: {str(e)}")

with tab2:
    uploaded_file = st.file_uploader("Ø§Ø±Ø¯Ùˆ Ø®Ø¨Ø± Ú©ÛŒ ÙØ§Ø¦Ù„ Ø§Ù¾Ù„ÙˆÚˆ Ú©Ø±ÛŒÚº", type=["txt", "pdf"])
    
    if uploaded_file is not None:
        with st.spinner("ÙØ§Ø¦Ù„ Ú©Ùˆ Ù¾Ú‘Ú¾Ø§ Ø¬Ø§ Ø±ÛØ§ ÛÛ’..."):
            text = extract_text_from_file(uploaded_file)
            
            if text:
                with st.spinner("Ø®Ù„Ø§ØµÛ Ø¨Ù†Ø§ÛŒØ§ Ø¬Ø§ Ø±ÛØ§ ÛÛ’..."):
                    summary = get_summary(text, model=summarizer, tokenizer=sum_tokenizer)
                    # issues = validate_summary(summary, original_text=article.text)

                    # if issues:
                    #     st.warning("Ø®Ù„Ø§ØµÛ’ Ù…ÛŒÚº Ú©Ú†Ú¾ Ù…Ù…Ú©Ù†Û Ù…Ø³Ø§Ø¦Ù„ Ù¾Ø§Ø¦Û’ Ú¯Ø¦Û’:")
                    #     for issue in issues:
                    #         st.markdown(f"- âš ï¸ {issue}")
                    # else:
                    #     st.success("Ø®Ù„Ø§ØµÛ Ù…Ø­ÙÙˆØ¸ Ø§ÙˆØ± Ù…Ù†Ø§Ø³Ø¨ ÛÛ’Û”")

                    st.subheader("Ø¯Ø³ØªØ§ÙˆÛŒØ² Ú©Ø§ Ø®Ù„Ø§ØµÛ:")
                    st.write(summary)
                
                with st.spinner("Ø¬Ø°Ø¨Ø§ØªÛŒ ØªØ¬Ø²ÛŒÛ Ú©ÛŒØ§ Ø¬Ø§ Ø±ÛØ§ ÛÛ’..."):
                    sentiment, score = get_sentiment(text,model=classifier, tokenizer=class_tokenizer)
                    st.subheader("Ø¬Ø°Ø¨Ø§ØªÛŒ ØªØ¬Ø²ÛŒÛ:")
                    st.write(f"ÛŒÛ Ø¯Ø³ØªØ§ÙˆÛŒØ² **{sentiment}** ÛÛ’")
                    # st.progress(score)

with tab3:
    text_input = st.text_area("Ø§Ø±Ø¯Ùˆ Ù…ØªÙ† Ø¯Ø§Ø®Ù„ Ú©Ø±ÛŒÚº", height=300)
    analyze_button = st.button("ØªØ¬Ø²ÛŒÛ Ú©Ø±ÛŒÚº")
    
    if analyze_button and text_input:
        with st.spinner("Ø®Ù„Ø§ØµÛ Ø¨Ù†Ø§ÛŒØ§ Ø¬Ø§ Ø±ÛØ§ ÛÛ’..."):
            summary = get_summary(text_input, model=summarizer, tokenizer=sum_tokenizer)
            # issues = validate_summary(summary, original_text=article.text)

            # if issues:
            #     st.warning("Ø®Ù„Ø§ØµÛ’ Ù…ÛŒÚº Ú©Ú†Ú¾ Ù…Ù…Ú©Ù†Û Ù…Ø³Ø§Ø¦Ù„ Ù¾Ø§Ø¦Û’ Ú¯Ø¦Û’:")
            #     for issue in issues:
            #         st.markdown(f"- âš ï¸ {issue}")
            # else:
            #     st.success("Ø®Ù„Ø§ØµÛ Ù…Ø­ÙÙˆØ¸ Ø§ÙˆØ± Ù…Ù†Ø§Ø³Ø¨ ÛÛ’Û”")

            st.subheader("Ù…ØªÙ† Ú©Ø§ Ø®Ù„Ø§ØµÛ:")
            st.write(summary)
        
        with st.spinner("Ø¬Ø°Ø¨Ø§ØªÛŒ ØªØ¬Ø²ÛŒÛ Ú©ÛŒØ§ Ø¬Ø§ Ø±ÛØ§ ÛÛ’..."):
            sentiment, score = get_sentiment(text_input, model=classifier, tokenizer=class_tokenizer)
            st.subheader("Ø¬Ø°Ø¨Ø§ØªÛŒ ØªØ¬Ø²ÛŒÛ:")
            st.write(f"ÛŒÛ Ù…ØªÙ† **{sentiment}** ÛÛ’")
            # st.progress(score)

# Add a sidebar for model configuration
with st.sidebar:
    st.header("Ù…Ø§ÚˆÙ„ Ú©ÛŒ ØªØ±ØªÛŒØ¨Ø§Øª")
    st.info("ÛŒÛ Ø§ÛŒÙ¾Ù„ÛŒÚ©ÛŒØ´Ù† Ø§Ø±Ø¯Ùˆ Ù†ÛŒÙˆØ² Ú©Ø§ Ø®Ù„Ø§ØµÛ Ø§ÙˆØ± Ø¬Ø°Ø¨Ø§ØªÛŒ ØªØ¬Ø²ÛŒÛ Ú©Ø±ØªÛŒ ÛÛ’ ")
    st.write("**Ø§ÛÙ… ÛØ¯Ø§ÛŒØ§Øª:**")
    st.write("1. Ù¾ÛÙ„ÛŒ Ø¨Ø§Ø± Ú†Ù„Ø§Ù†Û’ Ù¾Ø±ØŒ Ù…Ø§ÚˆÙ„ ÚˆØ§Ø¤Ù† Ù„ÙˆÚˆ Ú©Ø±Ù†Û’ Ù…ÛŒÚº Ú†Ù†Ø¯ Ù…Ù†Ù¹ Ù„Ú¯ Ø³Ú©ØªÛ’ ÛÛŒÚº")
    st.write("2. Ø¨Ú‘Û’ Ù…ØªÙ† Ú©Û’ ØªØ¬Ø²ÛŒÛ’ Ù…ÛŒÚº Ø²ÛŒØ§Ø¯Û ÙˆÙ‚Øª Ù„Ú¯ Ø³Ú©ØªØ§ ÛÛ’")
    # st.write("3. Ø¨Ø±Ø§Û Ø±Ø§Ø³Øª Ø§Ø±Ø¯Ùˆ Ù…ØªÙ† Ø³Ø¨ Ø³Û’ Ø¨ÛØªØ± Ù†ØªØ§Ø¦Ø¬ Ø¯ÛŒØªØ§ ÛÛ’")

# Footer
st.markdown("---")
st.caption("Ø§Ø±Ø¯Ùˆ Ù†ÛŒÙˆØ² Ø§ÛŒÚ©Ø³Ù¾Ø±ÛŒØ³ - Ø®Ø¨Ø±ÙˆÚº Ú©Ø§ Ø®Ù„Ø§ØµÛ Ø§ÙˆØ± Ø¬Ø°Ø¨Ø§ØªÛŒ ØªØ¬Ø²ÛŒÛ")